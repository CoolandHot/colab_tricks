{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoolandHot/colab_tricks/blob/main/spark_cluster_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to : [Tutorial on setting up Spark Cluster](https://medium.com/@jootorres_11979/how-to-install-and-set-up-an-apache-spark-cluster-on-hadoop-18-04-b4d70650ed42)"
      ],
      "metadata": {
        "id": "XK0mB-KzJOep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the tunnel to setup SSH remote hosting & install Spark, Hadoop, Scala, Java"
      ],
      "metadata": {
        "id": "sYQBTBfwu8uY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOMfNLD_zpf",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "08d65996-c777-471a-98c8-5190ea349826"
      },
      "source": [
        "#@title Colab-ssh tunnel\n",
        "#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n",
        "\n",
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"123456\" #@param {type: \"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)\n",
        "\n",
        "# Optional: if you want to clone a Github or Gitlab repository\n",
        "# repository_url=\"<PUT_YOUR_REPOSITORY_URL_HERE>\" #@param {type: \"string\"}\n",
        "# init_git_cloudflared(repository_url)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab_ssh\n",
            "  Downloading colab_ssh-0.3.27-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: colab-ssh\n",
            "Successfully installed colab-ssh-0.3.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "*{\n",
              "\toutline:none;\n",
              "}\n",
              "code{\n",
              "\tdisplay:inline-block;\n",
              "\tpadding:5px 10px;\n",
              "\tbackground: #444;\n",
              "\tborder-radius: 4px;\n",
              "\twhite-space: pre-wrap;\n",
              "\tposition:relative;\n",
              "\tcolor:white;\n",
              "}\n",
              ".copy-code-button{\n",
              "\tfloat:right;\n",
              "\tbackground:#333;\n",
              "\tcolor:white;\n",
              "\tborder: none;\n",
              "\tmargin: 0 0 0 10px;\n",
              "\tcursor: pointer;\n",
              "}\n",
              "p, li{\n",
              "\tmax-width:700px;\n",
              "}\n",
              ".choices{\n",
              "\tdisplay:flex;\n",
              "\tflex: 1 0 auto;\n",
              "}\n",
              ".choice-section{\n",
              "\tborder:solid 1px #555;\n",
              "\tborder-radius: 4px;\n",
              "\tmin-width:300px;\n",
              "\tmargin: 10px 15px 0 0;\n",
              "\tpadding: 0 15px 15px 15px ;\n",
              "}\n",
              ".button{\n",
              "\tpadding: 10px 15px;\n",
              "\tbackground:#333;\n",
              "\tborder-radius: 4px;\n",
              "\tborder:solid 1px #555;\n",
              "\tcolor:white;\n",
              "\tfont-weight:bold;\n",
              "\tcursor:pointer;\n",
              "}\n",
              ".pill{\n",
              "\tpadding:2px 4px;\n",
              "\tborder-radius: 100px;\n",
              "\tbackground-color:#e65858;\n",
              "\tfont-size:12px;\n",
              "\tfont-weight:bold;\n",
              "\tmargin: 0 15px;\n",
              "\tcolor:white;\n",
              "}\n",
              "</style>\n",
              "<details class=\"choice-section\">\n",
              "\t<summary style=\"cursor:pointer\">\n",
              "\t\t<h3 style=\"display:inline-block;margin-top:15px\">⚙️ Client machine configuration<span class=\"pill\">Required</span></h3>\n",
              "\t</summary>\n",
              "\t<p>Don't worry, you only have to do this <b>once per client machine</b>.</p>\n",
              "\t<ol>\n",
              "\t\t<li>Download <a href=\"https://developers.cloudflare.com/argo-tunnel/getting-started/installation\">Cloudflared (Argo Tunnel)</a>, then copy the absolute path of the cloudflare binary</li>\n",
              "\t\t<li>Now, you have to append the following to your SSH config file (usually under ~/.ssh/config), and make sure you replace the placeholder with the path you copied in Step 1:</li>\n",
              "\t</ol>\n",
              "\t<code>Host *.trycloudflare.com\n",
              "\tHostName %h\n",
              "\tUser root\n",
              "\tPort 22\n",
              "\tProxyCommand &ltPUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&gt access ssh --hostname %h\n",
              "\t</code>\n",
              "</details>\n",
              "<div class=\"choices\">\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>SSH Terminal</h4>\n",
              "\t\t<p>To connect using your terminal, type this command:</p>\n",
              "\t\t<code>ssh narrow-apps-fewer-decor.trycloudflare.com</code>\n",
              "\t</div>\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>VSCode Remote SSH</h4>\n",
              "\t\t<p>You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:</p>\n",
              "\t\t<code>narrow-apps-fewer-decor.trycloudflare.com</code>\n",
              "\t</div>\n",
              "</div>\n",
              "\n",
              "<script>\n",
              "// Copy any string\n",
              "function fallbackCopyTextToClipboard(text) {\n",
              "  var textArea = document.createElement(\"textarea\");\n",
              "  textArea.value = text;\n",
              "  \n",
              "  // Avoid scrolling to bottom\n",
              "  textArea.style.top = \"0\";\n",
              "  textArea.style.left = \"0\";\n",
              "  textArea.style.position = \"fixed\";\n",
              "\n",
              "  document.body.appendChild(textArea);\n",
              "  textArea.focus();\n",
              "  textArea.select();\n",
              "\n",
              "  try {\n",
              "    var successful = document.execCommand('copy');\n",
              "    var msg = successful ? 'successful' : 'unsuccessful';\n",
              "    console.log('Fallback: Copying text command was ' + msg);\n",
              "  } catch (err) {\n",
              "    console.error('Fallback: Oops, unable to copy', err);\n",
              "  }\n",
              "\n",
              "  document.body.removeChild(textArea);\n",
              "}\n",
              "\n",
              "// Show the copy button with every code tag\n",
              "document.querySelectorAll('code').forEach(function (codeBlock) {\n",
              "\tconst codeToCopy= codeBlock.innerText;\n",
              "\tvar pre = document.createElement('pre');\n",
              "\tpre.innerText = codeToCopy;\n",
              "    var button = document.createElement('button');\n",
              "    button.className = 'copy-code-button';\n",
              "    button.type = 'button';\n",
              "    button.innerText = 'Copy';\n",
              "\tbutton.onclick = function(){\n",
              "\t\tfallbackCopyTextToClipboard(codeToCopy);\n",
              "\t\tbutton.innerText = 'Copied'\n",
              "\t\tsetTimeout(()=>{\n",
              "\t\t\tbutton.innerText = 'Copy'\n",
              "\t\t},2000)\n",
              "\t}\n",
              "\tcodeBlock.children = pre;\n",
              "\tcodeBlock.prepend(button)\n",
              "});\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Spark, Hadoop, Scala, Java\n",
        "%%capture\n",
        "\n",
        "!pip install pyspark\n",
        "# findspark will locate Spark on the system and import it as a regular library.\n",
        "!pip install -q findspark\n",
        "\n",
        "# ping tool\n",
        "!apt-get install iputils-ping\n",
        "# SSH\n",
        "!apt-get install openssh-server openssh-client\n",
        "\n",
        "# Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "Path_of_JAVA_installation = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "# Scala\n",
        "!apt-get install scala\n",
        "# Apache Spark with Hadoop\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!mv spark-3.2.0-bin-hadoop3.2 /usr/local/spark\n",
        "\n",
        "# ngrok\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "\n",
        "# set ssh config ProxyCommand\n",
        "!mkdir -p ~/.ssh/\n",
        "!touch ~/.ssh/config\n",
        "!chmod 600 ~/.ssh/config\n",
        "with open('/root/.ssh/config', 'w') as file_object:\n",
        "    file_object.write('''Host *.trycloudflare.com\n",
        "\tHostName %h\n",
        "\tUser root\n",
        "\tPort 22\n",
        "\tProxyCommand /content/cloudflared access ssh --hostname %h\n",
        "    StrictHostKeyChecking no\n",
        "    ''')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3q9P4tIMwxdD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below are only required in the master host"
      ],
      "metadata": {
        "id": "pyIUikUBTyDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_host = \"masterHostForSpark\"\n",
        "MASTER_URL = 'narrow-apps-fewer-decor.trycloudflare.com' #@param {type:\"string\"}\n",
        "MASTER_IP = !hostname -I\n",
        "MASTER_IP = MASTER_IP[0].replace(' ', '')\n",
        "\n",
        "slave_num = 3 #@param {type:\"integer\"}\n",
        "Slave0_host = \"james-assistance-fuji-module.trycloudflare.com\" #@param {type:\"string\"}\n",
        "Slave1_host = \"catalogs-grass-covers-conduct.trycloudflare.com\" #@param {type:\"string\"}\n",
        "Slave2_host = \"davis-circular-share-knife.trycloudflare.com\" #@param {type:\"string\"}\n",
        "Slave3_host = \"high-patrol-vertical-labour.trycloudflare.com\" #@param {type:\"string\"}\n",
        "Slave4_host = \"high-patrol-vertical-labour.trycloudflare.com\" #@param {type:\"string\"}\n",
        "Slave_hosts = [Slave0_host, Slave1_host, Slave2_host, Slave3_host, Slave4_host]\n",
        "\n",
        "\n",
        "# !cp /usr/local/spark/conf/spark-defaults.conf.template /usr/local/spark/conf/spark-defaults.conf\n",
        "# !mkdir -p /usr/local/spark/applicationHistory\n",
        "# with open('/usr/local/spark/conf/spark-defaults.conf', 'w') as file_object:\n",
        "#     file_object.write(f'''\n",
        "# spark.master                     spark://{MASTER_host}:7077\n",
        "# spark.eventLog.enabled           true\n",
        "# # spark.eventLog.dir               hdfs://namenode:8021/directory\n",
        "# spark.eventLog.dir               file:///usr/local/spark/logs\n",
        "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
        "# spark.driver.memory              5g\n",
        "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n",
        "# spark.history.fs.logDirectory    file:///usr/local/spark/applicationHistory\n",
        "# spark.history.fs.update.interval 30s\n",
        "# spark.history.provider           org.apache.spark.deploy.history.FsHistoryProvider \n",
        "# ''')\n",
        "\n",
        "# !cp /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh\n",
        "with open('/usr/local/spark/conf/spark-env.sh', 'w') as file_object:\n",
        "    file_object.write(f'SPARK_MASTER_HOST=\"{MASTER_host}\"')\n",
        "\n",
        "with open('/usr/local/spark/conf/slaves', 'w') as file_object:\n",
        "    file_object.write(f'\\n{MASTER_host}\\n')\n",
        "    for i in range(slave_num):\n",
        "        file_object.write(f'{Slave_hosts[i]}\\n')\n",
        "\n",
        "\n",
        "# check hostname to MASTER_host\n",
        "# !hostname {MASTER_host}\n",
        "with open('/etc/hosts', 'w') as file_object:\n",
        "    file_object.write(f'''127.0.0.1\tlocalhost\n",
        " ::1\tlocalhost ip6-localhost ip6-loopback\n",
        " fe00::0\tip6-localnet\n",
        " ff00::0\tip6-mcastprefix\n",
        " ff02::1\tip6-allnodes\n",
        " ff02::2\tip6-allrouters\n",
        " {MASTER_IP}\t{MASTER_host}\n",
        " ''')\n",
        "\n",
        "\n",
        "with open('/root/.bashrc', 'a') as file_object:\n",
        "    file_object.write(f'''\n",
        "export JAVA_HOME={Path_of_JAVA_installation}\n",
        "export PATH=$PATH:$JAVE_HOME/bin\n",
        "export SPARK_HOME=/usr/local/spark\n",
        "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "''')\n",
        "# sourcing the ~/.bashrc file\n",
        "!source ~/.bashrc\n",
        "\n",
        "\n",
        "\n",
        "# create rsa key pairs\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
        "!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "\n",
        "from google.colab import files\n",
        "rsa_files = 'rsa_files.zip'\n",
        "!zip -r {rsa_files} ~/.ssh/id_rsa ~/.ssh/id_rsa.pub\n",
        "files.download(rsa_files)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FYUy00LP8vvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !scp /usr/local/spark/conf/spark-defaults.conf root@{Slave0_host}:/usr/local/spark/conf\n",
        "# !scp /usr/local/spark/conf/spark-defaults.conf root@{Slave1_host}:/usr/local/spark/conf\n",
        "# !scp /usr/local/spark/conf/slaves root@{Slave0_host}:/usr/local/spark/conf\n",
        "# !scp /usr/local/spark/conf/slaves root@{Slave1_host}:/usr/local/spark/conf"
      ],
      "metadata": {
        "id": "cxdTzEDDOELx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy public key to the **slave** host\n",
        "1. [from local machine on powershell]\n",
        "```\n",
        "$slave = \"<slave_host>\"\n",
        "type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh $slave \"cat >> .ssh/authorized_keys\"\n",
        "```\n",
        "After entering the host's password, it's set properly. Now you can login with `ssh $slave` directly. \n",
        "\n",
        "BTW: if you push your local `id_rsa` to remote using the same method, its permission (Permissions 0644) should be changed to (Permissions 0400): `chmod 400 ~/.ssh/id_rsa`\n",
        "\n",
        "2. [inside master_host]\n",
        "```\n",
        "!ssh-copy-id <slave_host>\n",
        "```\n"
      ],
      "metadata": {
        "id": "d_vtK_imttEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can test the connection with (type `exit` to disconnect):"
      ],
      "metadata": {
        "id": "uE-9g1Pv0KY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ssh -o ProxyCommand=\"/content/cloudflared access ssh --hostname %h\" {Slave0_host}\n",
        "!ssh {Slave0_host}"
      ],
      "metadata": {
        "id": "C-xdllDww3S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now start the master and worker hosts"
      ],
      "metadata": {
        "id": "3U-jfS8EVHre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/spark/sbin/start-all.sh\n",
        "# !/usr/local/spark/sbin/stop-all.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJG3IOVqB9qx",
        "outputId": "e0b97084-485f-4010-e5c4-e7c3fa0df520"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark--org.apache.spark.deploy.master.Master-1-174f4a950415.out\n",
            "masterHostForSpark: Warning: Permanently added 'masterhostforspark,172.28.0.2' (ECDSA) to the list of known hosts.\n",
            "masterHostForSpark: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-174f4a950415.out\n",
            "catalogs-grass-covers-conduct.trycloudflare.com: Warning: Permanently added 'catalogs-grass-covers-conduct.trycloudflare.com' (ECDSA) to the list of known hosts.\n",
            "davis-circular-share-knife.trycloudflare.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-505e4e27e3a5.out\n",
            "james-assistance-fuji-module.trycloudflare.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-355709b4fafc.out\n",
            "catalogs-grass-covers-conduct.trycloudflare.com: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-b173ea3e5702.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check to if the services started\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O58BbTMEEWF",
        "outputId": "c7ff714f-479a-45ab-ec22-3ebb30539ce9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2361 Jps\n",
            "2153 Master\n",
            "2252 Worker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the log outputs\n",
        "logs = !ls /usr/local/spark/logs/\n",
        "logs = logs.s.split()\n",
        "for log_file in logs:\n",
        "    !cat /usr/local/spark/logs/{log_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc4N74__YkEU",
        "outputId": "c55061f6-869d-4a7d-ac77-9b666aa13841"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Command: /usr/lib/jvm/java-11-openjdk-amd64/bin/java -cp /usr/local/spark/conf/:/usr/local/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host masterHostForSpark --port 7077 --webui-port 8080\n",
            "========================================\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "22/01/22 01:02:31 INFO Master: Started daemon with process name: 2153@174f4a950415\n",
            "22/01/22 01:02:31 INFO SignalUtils: Registering signal handler for TERM\n",
            "22/01/22 01:02:31 INFO SignalUtils: Registering signal handler for HUP\n",
            "22/01/22 01:02:31 INFO SignalUtils: Registering signal handler for INT\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "22/01/22 01:02:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/01/22 01:02:32 INFO SecurityManager: Changing view acls to: root\n",
            "22/01/22 01:02:32 INFO SecurityManager: Changing modify acls to: root\n",
            "22/01/22 01:02:32 INFO SecurityManager: Changing view acls groups to: \n",
            "22/01/22 01:02:32 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/01/22 01:02:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/01/22 01:02:33 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\n",
            "22/01/22 01:02:33 INFO Master: Starting Spark master at spark://masterHostForSpark:7077\n",
            "22/01/22 01:02:33 INFO Master: Running Spark version 3.2.0\n",
            "22/01/22 01:02:34 WARN Utils: Service 'MasterUI' could not bind on port 8080. Attempting port 8081.\n",
            "22/01/22 01:02:34 INFO Utils: Successfully started service 'MasterUI' on port 8081.\n",
            "22/01/22 01:02:34 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://masterHostForSpark:8081\n",
            "22/01/22 01:02:34 INFO Master: I have been elected leader! New state: ALIVE\n",
            "22/01/22 01:02:37 INFO Master: Registering worker 172.28.0.2:36719 with 2 cores, 11.7 GiB RAM\n",
            "Spark Command: /usr/lib/jvm/java-11-openjdk-amd64/bin/java -cp /usr/local/spark/conf/:/usr/local/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://masterHostForSpark:7077\n",
            "========================================\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "22/01/22 01:02:35 INFO Worker: Started daemon with process name: 2252@174f4a950415\n",
            "22/01/22 01:02:35 INFO SignalUtils: Registering signal handler for TERM\n",
            "22/01/22 01:02:35 INFO SignalUtils: Registering signal handler for HUP\n",
            "22/01/22 01:02:35 INFO SignalUtils: Registering signal handler for INT\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "22/01/22 01:02:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/01/22 01:02:36 INFO SecurityManager: Changing view acls to: root\n",
            "22/01/22 01:02:36 INFO SecurityManager: Changing modify acls to: root\n",
            "22/01/22 01:02:36 INFO SecurityManager: Changing view acls groups to: \n",
            "22/01/22 01:02:36 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/01/22 01:02:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/01/22 01:02:36 INFO Utils: Successfully started service 'sparkWorker' on port 36719.\n",
            "22/01/22 01:02:36 INFO Worker: Worker decommissioning not enabled.\n",
            "22/01/22 01:02:37 INFO Worker: Starting Spark worker 172.28.0.2:36719 with 2 cores, 11.7 GiB RAM\n",
            "22/01/22 01:02:37 INFO Worker: Running Spark version 3.2.0\n",
            "22/01/22 01:02:37 INFO Worker: Spark home: /usr/local/spark\n",
            "22/01/22 01:02:37 INFO ResourceUtils: ==============================================================\n",
            "22/01/22 01:02:37 INFO ResourceUtils: No custom resources configured for spark.worker.\n",
            "22/01/22 01:02:37 INFO ResourceUtils: ==============================================================\n",
            "22/01/22 01:02:37 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.\n",
            "22/01/22 01:02:37 INFO Utils: Successfully started service 'WorkerUI' on port 8082.\n",
            "22/01/22 01:02:37 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://masterHostForSpark:8082\n",
            "22/01/22 01:02:37 INFO Worker: Connecting to master masterHostForSpark:7077...\n",
            "22/01/22 01:02:37 INFO TransportClientFactory: Successfully created connection to masterHostForSpark/172.28.0.2:7077 after 59 ms (0 ms spent in bootstraps)\n",
            "22/01/22 01:02:37 INFO Worker: Successfully registered with master spark://masterHostForSpark:7077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(\"8081\")"
      ],
      "metadata": {
        "id": "UVTq3SFsahbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7175e2f4-b26a-4b01-ccbb-16c7fcdbb507"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8081, \"/\", \"https://localhost:8081/\", window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spark_cluster_setup",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}